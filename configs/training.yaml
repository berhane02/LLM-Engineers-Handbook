settings:
  docker:
    parent_image: 767398029096.dkr.ecr.us-east-2.amazonaws.com/llm-training-python310:latest
    skip_build: True
  orchestrator.sagemaker:
    synchronous: false

parameters:
  finetuning_type: sft
  num_train_epochs: 3
  per_device_train_batch_size: 2
  learning_rate: 3e-4
  dataset_huggingface_workspace: mlabonne
  is_dummy: true # Change this to 'false' to run the training with the full dataset and epochs.
